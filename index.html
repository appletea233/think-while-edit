<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation">
  <meta name="keywords" content="TWIG, Visual Generation, Multimodal Reasoning, AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Thinking-while-Generating Project Page</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <style>
    body { font-family: 'Noto Sans', sans-serif; background-color: #f5f5f5; }
    .hero-body { padding-bottom: 1rem; }
    .title.is-1 { font-family: 'Google Sans', sans-serif; font-weight: 700; line-height: 1.2; }
    .publication-title { margin-bottom: 1rem; }
    .publication-authors { font-size: 1.2rem; margin-bottom: 1rem; }
    .author-block { display: inline-block; margin-right: 15px; }
    .publication-links .button { margin: 5px; border-radius: 20px; }
    .teaser-section { padding: 2rem 0; }
    .teaser-image { width: 100%; border-radius: 10px; box-shadow: 0 5px 15px rgba(0,0,0,0.1); }
    .content h2 { text-align: center; font-family: 'Google Sans', sans-serif; margin-top: 3rem; margin-bottom: 1.5rem; }
    .content h3 { font-family: 'Google Sans', sans-serif; margin-top: 1.5rem; }
    .bibtex-section { background-color: #f5f5f5; padding: 20px; border-radius: 5px; overflow-x: auto; }
    .results-image { border-radius: 10px; margin: 10px 0; box-shadow: 0 0 10px rgba(0,0,0,0.05); }
    footer { background-color: #fff; padding: 3rem 1.5rem; margin-top: 3rem; }
    .method-box { background: #fff; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.05); margin-bottom: 20px;}
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <h1 class="title is-1 publication-title">
            Thinking-while-Generating:<br>
            Interleaving Textual Reasoning throughout Visual Generation
          </h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Author Name 1</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Author Name 2</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Author Name 3</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institution Name One,</span>
            <span class="author-block"><sup>2</sup>Institution Name Two</span>
          </div>

          <div class="publication-links">
            <span class="link-block">
              <a href="#" class="button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="#" class="button is-normal is-rounded is-dark">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
              </a>
            </span>
            <span class="link-block">
              <a href="#" class="button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code</span>
              </a>
            </span>
            <span class="link-block">
              <a href="#" class="button is-normal is-rounded is-dark">
                <span class="icon"><i class="far fa-images"></i></span>
                <span>TWIG-50K Data</span>
              </a>
            </span>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light teaser-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img src="static/images/teaser.png" class="teaser-image" alt="Thinking-while-Generating Overview" onerror="this.style.display='none'; this.insertAdjacentHTML('afterend', '<div style=\'background:#ddd;height:300px;display:flex;align-items:center;justify-content:center;border-radius:10px;\'>[Insert Teaser Image / Figure 1 Here]</div>');"/>
        <h2 class="subtitle has-text-centered" style="margin-top: 15px;">
          <b>Thinking-while-Generating (TWIG)</b> interleaves textual reasoning (Thinking) with visual synthesis (Generating) and region-level refinement (Reflecting) in a single unified framework.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            [cite_start]Recent advances in visual generation have increasingly explored the integration of reasoning capabilities[cite: 43]. [cite_start]However, existing methods typically incorporate textual reasoning either before generation (as pre-planning) or after (as post-refinement), lacking on-the-fly multimodal interaction during the generation process itself[cite: 44].
          </p>
          <p>
            [cite_start]To address this, we introduce <b>Thinking-while-Generating (TWIG)</b>, the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process[cite: 45]. [cite_start]As the visual content is progressively generated, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones[cite: 46]. [cite_start]We investigate three strategies to implement this framework: zero-shot prompting, supervised fine-tuning (SFT) on our curated <b>TWIG-50K</b> dataset, and reinforcement learning via a customized <b>TWIG-GRPO</b> strategy[cite: 48]. [cite_start]Our experiments demonstrate that TWIG produces more context-aware and semantically rich visual outputs compared to existing baselines[cite: 47].
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="background-color: #ffffff;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">The TWIG Framework</h2>
        <div class="content has-text-justified">
          <p>
            [cite_start]Our framework interleaves textual reasoning with visual generation through three core schemes, implemented using a Unified Language-Model (ULM)[cite: 276]:
          </p>
          
          <div class="columns is-multiline">
            <div class="column is-4">
              <div class="method-box">
                <h3 class="title is-5"><i class="far fa-clock"></i> When to Think</h3>
                <p>
                  [cite_start]The model determines an interleaved reasoning schedule, decoupling generation into smaller, controllable sub-tasks (visual regions)[cite: 281]. [cite_start]We utilize a static schedule ($K=3$) targeting upper, central, and lower image regions[cite: 301].
                </p>
              </div>
            </div>
            <div class="column is-4">
              <div class="method-box">
                <h3 class="title is-5"><i class="far fa-comment-dots"></i> What to Say</h3>
                <p>
                  [cite_start]At each step, the model generates a textual thought serving as a localized sub-prompt[cite: 303]. [cite_start]This thought is conditioned on the input prompt and previous visual content, providing fine-grained guidance for the next segment[cite: 304].
                </p>
              </div>
            </div>
            <div class="column is-4">
              <div class="method-box">
                <h3 class="title is-5"><i class="fas fa-sync-alt"></i> How to Refine</h3>
                <p>
                  [cite_start]After generating a region, the model performs an immediate "reflection" step[cite: 409]. It generates a critic score and a revised thought. [cite_start]If the score is low, it triggers local re-generation to correct errors on the fly[cite: 411].
                </p>
              </div>
            </div>
          </div>

          <div style="text-align: center; margin-top: 20px;">
             <img src="static/images/method.png" class="results-image" alt="Method Overview" onerror="this.style.display='none'; this.insertAdjacentHTML('afterend', '<div style=\'background:#eee;height:250px;display:flex;align-items:center;justify-content:center;border-radius:10px;\'>[Insert Method Figure / Figure 3 Here]</div>');"/>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Training Strategies</h2>
    <div class="content has-text-justified">
      [cite_start]<p>We explore three progressive implementation routes to unlock the potential of interleaved reasoning[cite: 175]:</p>
      <ul>
        <li>
          [cite_start]<b>Zero-Shot Prompting (TWIG-ZS):</b> We craft interleave-aware prompts to elicit global plans and reasoning thoughts without parameter updates[cite: 176].
        </li>
        <li>
          [cite_start]<b>Supervised Fine-Tuning (TWIG-SFT):</b> We curated <b>TWIG-50K</b>, a high-quality dataset with 50,000 samples decomposed into thinking, generation, and reflection tasks[cite: 178].
        </li>
        <li>
          [cite_start]<b>Reinforcement Learning (TWIG-RL):</b> We employ a <b>TWIG-GRPO</b> algorithm to optimize the policy using a reward ensemble (Human Preference, Object Grounding, VQA, and LMM Alignment)[cite: 179, 713].
        </li>
      </ul>
    </div>
  </div>
</section>

<section class="section" style="background-color: #ffffff;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Experimental Results</h2>
        
        <h3 class="title is-4">Quantitative Analysis</h3>
        <div class="content has-text-justified">
          <p>
            [cite_start]Experiments on <b>T2I-CompBench</b> show that TWIG significantly outperforms the baseline Janus-Pro-7B[cite: 259].
          </p>
          <ul>
            [cite_start]<li><b>Zero-Shot:</b> TWIG-ZS achieves <b>+9.52%</b> improvement in Attribute Binding (Color) and <b>+12.57%</b> in Complex settings compared to the baseline[cite: 476].</li>
            [cite_start]<li><b>RL Enhancement:</b> TWIG-RL further pushes the boundary, achieving substantial gains across spatial and attribute binding metrics (e.g., exceeding <b>+5%</b> improvement over SFT)[cite: 716].</li>
          </ul>
        </div>

        <h3 class="title is-4">Qualitative Comparison</h3>
        <div class="content has-text-justified">
          <p>
            Our method demonstrates progressive improvements in compositional fidelity, object counting, and visual realism. [cite_start]The reflection mechanism effectively corrects spatial misalignments and shadow inconsistencies during the generation process[cite: 790, 791].
          </p>
          <img src="static/images/results.png" class="results-image" alt="Qualitative Results" onerror="this.style.display='none'; this.insertAdjacentHTML('afterend', '<div style=\'background:#eee;height:300px;display:flex;align-items:center;justify-content:center;border-radius:10px;\'>[Insert Qualitative Comparison / Figure 5 Here]</div>');"/>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{anonymous2025twig,
  title={Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation},
  author={Anonymous Authors},
  journal={CVPR Submission 17669},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website template based on <a href="https://nerfies.github.io/">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
